---
sidebar_label: 'Intelligent Context Condensing'
---
import Codicon from '@site/src/components/Codicon';

# Intelligent Context Condensing

## How It Works

As your conversation with Saarthi grows, it might approach the context window limit of the underlying AI model. When this happens, older messages would typically be removed to make space. Intelligent Context Condensing aims to prevent this abrupt loss by:

1.  **Summarizing:** Using an AI model, it condenses earlier parts of the conversation.
2.  **Retaining Essentials:** The goal is to reduce the overall token count while keeping the key information from the summarized messages.
3.  **Maintaining Flow:** This allows the AI to have a more coherent understanding of the entire conversation, even very long ones.

**Important Considerations:**
*   **Summarization Impact:** While original messages are preserved if you use [Checkpoints](/features/checkpoints) to rewind, the summarized version is what's used in ongoing LLM calls to keep the context manageable.
*   **Cost:** The AI call to perform the summarization incurs a cost. This cost is included in the context condensing metrics displayed in the UI.

## Configuration

Intelligent Context Condensing is **enabled by default** and offers several configuration options:

1.  Open Saarthi settings (<Codicon name="gear" /> icon in the top right corner of the Saarthi panel).
2.  Navigate to the "Context" settings section.
3.  Configure the available options:
    - **Automatically trigger intelligent context condensing**: Enabled by default, this controls whether condensing happens automatically
    - **Threshold to trigger intelligent context condensing**: A percentage slider (default 100%) that determines when condensing activates based on context window usage
    - **API Configuration for Context Condensing**: Choose which API configuration to use for condensing operations (defaults to your current active configuration)
    - **Custom Context Condensing Prompt**: Customize the system prompt used for context condensing operations

<img src="/static/img/intelligent-context-condensing/intelligent-context-condensing.png" alt="Settings for Intelligent Context Condensing" width="600" />
*Intelligent Context Condensing configuration options: automatic triggering toggle, threshold slider, API configuration selection, and custom prompt customization.*
## Controlling and Understanding Context Condensing

Saarthi provides several ways to control and understand the Intelligent Context Condensing feature:

### Controlling Context Condensing
*   **Automatic Threshold:** The threshold slider in "Context" settings allows you to define a percentage (e.g., 80%) of context window usage. Saarthi will attempt to condense the context automatically when the conversation reaches this level of capacity.
*   **API Configuration:** Select which API configuration to use for context condensing operations. This allows you to use a different provider or model specifically for condensing if desired.
*   **Custom Prompts:** Modify the system prompt used for condensing to better suit your workflow or to emphasize certain aspects of conversation summarization.
*   **Manual Trigger:** A **Condense Context** button is available at the top of the task, positioned to the right of the context bar. This allows you to initiate the context condensing process at any time.

    <img src="/static/img/intelligent-context-condensing/intelligent-context-condensing-1.png" alt="Manual Condense Context button in expanded task view" width="600" />
    *The Manual Condense Context button (highlighted with a yellow arrow) is easily accessible for manual control.*

### Understanding Context Condensing Activity
*   **Context Condensing Metrics:** When context condensing occurs, Saarthi displays:
    *   The context token counts before and after context condensing.
    *   The cost associated with the context condensing AI call.
    *   An expandable summary detailing what was condensed (this information is part of the `ContextCondenseRow` component visible in the chat history).

<img src="/static/img/intelligent-context-condensing/intelligent-context-condensing-2.png" alt="Context condensed message in chat" width="600" />
*After context condensing, a message indicates the context has been condensed, showing token changes and cost.*

*   **Visual Indicators:**
    *   A progress indicator ("Condensing context...") is shown in the chat interface while context condensing is active.

<img src="/static/img/intelligent-context-condensation/intelligent-context-condensation-3.png" alt="Condensing context progress indicator in chat" width="600" />
*The "Condensing context..." indicator appears in the chat during the process.*

    *   The task header also displays the current context condensing status.
    *   The `ContextWindowProgress` bar offers a visual representation of token distribution, including current usage, space reserved for the AI's output, available space, and raw token numbers.
*   **Interface Clarity:** The "Condense Context" button includes a tooltip explaining its function, available in all supported languages.

## Technical Implementation

### Token Counting
Saarthi uses a sophisticated token counting system that:
- Employs native token counting endpoints when available (e.g., Anthropic's API)
- Falls back to tiktoken estimation if API calls fail
- Provides accurate counting for different content types:
  - Text content: Uses word-based estimation with punctuation and newline overhead
  - Image content: Uses a conservative estimate of 300 tokens per image
  - System prompts: Includes additional overhead for structural elements

### Context Window Management
- By default, 30% of the context window is reserved (20% for model output and 10% as a safety buffer), leaving 70% available for conversation history.
- This reservation can be overridden by model-specific settings
- The system automatically calculates available space while maintaining this reservation

